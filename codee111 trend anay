DB_SERVER=SRVWUPNQ0986V
DB_USER=GSOC_Test
DB_PASSWORD=Westernccure@2025
DB_DATABASE=ACVSCore

 SELECT AI.Image AS ImageBuffer
        FROM ACVSCore.Access.Images AI
        WHERE AI.ParentId = @id




# from fastapi import FastAPI, File, UploadFile, HTTPException
# from fastapi.middleware.cors import CORSMiddleware
# from fastapi.responses import JSONResponse
# import pandas as pd
# import numpy as np
# from io import BytesIO
# from dateutil import parser
# from datetime import datetime
# import re, zoneinfo

# app = FastAPI(title="Employee Travel Dashboard â€” Parser")

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=[ 
#         "http://localhost:5173",
#         "http://localhost:3000",
#         "http://127.0.0.1:5173",
#         "http://127.0.0.1:3000",
#     ],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# SERVER_TZ = zoneinfo.ZoneInfo("Asia/Kolkata")

# def normalize_and_parse(dt_val):
#     if pd.isna(dt_val):
#         return None
#     s = str(dt_val).strip()
#     s = re.sub(r"(\d{1,2})\.(\d{1,2})(?!\d)", r"\1:\2", s)
#     try:
#         dt = parser.parse(s, dayfirst=False)
#         if dt.tzinfo is None:
#             dt = dt.replace(tzinfo=SERVER_TZ)
#         return dt.astimezone(zoneinfo.ZoneInfo("UTC"))
#     except Exception:
#         return None

# @app.post("/upload")
# async def upload_excel(file: UploadFile = File(...)):
#     if not file.filename.lower().endswith((".xlsx", ".xls", ".csv")):
#         raise HTTPException(status_code=400, detail="Unsupported file type")

#     content = await file.read()
#     try:
#         if file.filename.lower().endswith(".csv"):
#             df = pd.read_csv(BytesIO(content))
#         else:
#             df = pd.read_excel(BytesIO(content))
#     except Exception as e:
#         raise HTTPException(status_code=400, detail=f"Failed to read spreadsheet: {e}")

#     expected = [
#         "AGENCY ID","AGENCY NAME","LAST NAME","FIRST NAME","TRAVELER",
#         "EMP ID","EMAIL","PNR","LEG TYPE","BEGIN DATE","FROM LOCATION","FROM COUNTRY",
#         "END DATE","TO LOCATION","TO COUNTRY"
#     ]

#     cols_map = {c.lower().strip(): c for c in df.columns}

#     def get_col(ci):
#         for k,v in cols_map.items():
#             if k == ci.lower():
#                 return v
#         for k,v in cols_map.items():
#             if ci.lower() in k:
#                 return v
#         return None

#     col_get = {c: get_col(c) for c in expected}

#     clean = {}
#     for canon, found in col_get.items():
#         clean[canon] = df[found] if found in df.columns else pd.Series([None]*len(df))

#     clean_df = pd.DataFrame(clean)
#     clean_df['BEGIN_DT'] = clean_df['BEGIN DATE'].apply(normalize_and_parse)
#     clean_df['END_DT'] = clean_df['END DATE'].apply(normalize_and_parse)

#     now_local = datetime.now(tz=SERVER_TZ)
#     now_utc = now_local.astimezone(zoneinfo.ZoneInfo('UTC'))

#     def is_active(row):
#         b, e = row['BEGIN_DT'], row['END_DT']
#         return bool(b and e and b <= now_utc <= e)

#     clean_df['active_now'] = clean_df.apply(is_active, axis=1)

#     # âœ… Replace problematic NaN/inf values
#     clean_df = clean_df.replace([np.nan, np.inf, -np.inf, pd.NaT], None)

#     # ðŸ§¹ Smart cleanup: remove empty and footer/metadata rows
#     import re

#     original_row_count = len(clean_df)

#     def is_footer_row(row):
#         combined = " ".join(str(v) for v in row.values if v is not None).lower()
#         footer_patterns = [
#             r"copyright", r"all rights reserved", r"gardaworld", r"utc",
#             r"\b\d{1,2}-[a-z]{3}-\d{4}\b"
#         ]
#         return any(re.search(p, combined) for p in footer_patterns)

#     # Drop completely empty rows
#     clean_df = clean_df.dropna(how="all")

#     # Drop rows that match footer patterns anywhere
#     clean_df = clean_df[~clean_df.apply(is_footer_row, axis=1)]

#     # Drop rows missing all key traveler identifiers
#     clean_df = clean_df[
#         clean_df["FIRST NAME"].notna() |
#         clean_df["LAST NAME"].notna() |
#         clean_df["EMAIL"].notna()
#     ]

#     removed_rows = original_row_count - len(clean_df)

#     def row_to_obj(i, row):
#         return {
#             'index': int(i),
#             'agency_id': row.get('AGENCY ID'),
#             'agency_name': row.get('AGENCY NAME'),
#             'first_name': row.get('FIRST NAME'),
#             'last_name': row.get('LAST NAME'),
#             'emp_id': row.get('EMP ID'),
#             'email': row.get('EMAIL'),
#             'pnr': row.get('PNR'),
#             'leg_type': row.get('LEG TYPE'),
#             'begin_dt': row.get('BEGIN_DT').isoformat() if row.get('BEGIN_DT') else None,
#             'end_dt': row.get('END_DT').isoformat() if row.get('END_DT') else None,
#             'from_location': row.get('FROM LOCATION'),
#             'from_country': row.get('FROM COUNTRY'),
#             'to_location': row.get('TO LOCATION'),
#             'to_country': row.get('TO COUNTRY'),
#             'active_now': bool(row.get('active_now'))
#         }

#     items = [row_to_obj(i, r) for i, r in clean_df.iterrows()]

#     summary = {
#         'rows_received': len(clean_df),
#         'rows_removed_as_footer_or_empty': removed_rows,
#         'rows_with_parse_errors': int(clean_df['BEGIN_DT'].isna().sum() + clean_df['END_DT'].isna().sum()),
#         'active_now_count': int(clean_df['active_now'].sum())
#     }

#     # âœ… Ensure JSON-safe output
#     return JSONResponse(content={'summary': summary, 'items': items})




















# ðŸ“ðŸ“ðŸ“ðŸ“ðŸ“ðŸ“ðŸ“ðŸ“ðŸ“ðŸ“ðŸ“ðŸ“ðŸ“ðŸ“ðŸ“ðŸ“  new code is â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸







from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import pandas as pd
import numpy as np
from io import BytesIO
from dateutil import parser
from datetime import datetime
import re, zoneinfo
import io

from fastapi import File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
import pandas as pd
import numpy as np
from io import BytesIO, StringIO
from dateutil import parser
from datetime import datetime
import re, zoneinfo, io
app = FastAPI(title="Employee Travel Dashboard â€” Parser")

app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "http://localhost:3000",
        "http://localhost:3001",
        "http://127.0.0.1:5173",
        "http://127.0.0.1:3000",
        "http://127.0.0.1:3001",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

SERVER_TZ = zoneinfo.ZoneInfo("Asia/Kolkata")

# âœ… Global variable to store previous data
previous_data = {
    "summary": None,
    "items": None
}

def normalize_and_parse(dt_val):
    if pd.isna(dt_val):
        return None
    s = str(dt_val).strip()
    s = re.sub(r"(\d{1,2})\.(\d{1,2})(?!\d)", r"\1:\2", s)
    try:
        dt = parser.parse(s, dayfirst=False)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=SERVER_TZ)
        return dt.astimezone(zoneinfo.ZoneInfo("UTC"))
    except Exception:
        return None

# ///////////////////////////////

# @app.post("/upload")
# async def upload_excel(file: UploadFile = File(None)):
#     global previous_data

#     # âœ… If no new file uploaded, return previous data
#     if file is None:
#         if previous_data["items"] is not None:
#             return JSONResponse(content={
#                 "summary": previous_data["summary"],
#                 "items": previous_data["items"],
#                 "message": "Returned previously uploaded data (no new file uploaded)"
#             })
#         else:
#             raise HTTPException(status_code=400, detail="No file uploaded and no previous data found.")

#     if not file.filename.lower().endswith((".xlsx", ".xls", ".csv")):
#         raise HTTPException(status_code=400, detail="Unsupported file type")

#     content = await file.read()
#     try:
#         if file.filename.lower().endswith(".csv"):
#             df = pd.read_csv(BytesIO(content))
#         else:
#             df = pd.read_excel(BytesIO(content))
#     except Exception as e:
#         raise HTTPException(status_code=400, detail=f"Failed to read spreadsheet: {e}")

#     expected = [
#         "AGENCY ID","AGENCY NAME","LAST NAME","FIRST NAME","TRAVELER",
#         "EMP ID","EMAIL","PNR","LEG TYPE","BEGIN DATE","FROM LOCATION","FROM COUNTRY",
#         "END DATE","TO LOCATION","TO COUNTRY"
#     ]

#     cols_map = {c.lower().strip(): c for c in df.columns}

#     def get_col(ci):
#         for k,v in cols_map.items():
#             if k == ci.lower():
#                 return v
#         for k,v in cols_map.items():
#             if ci.lower() in k:
#                 return v
#         return None

#     col_get = {c: get_col(c) for c in expected}

#     clean = {}
#     for canon, found in col_get.items():
#         clean[canon] = df[found] if found in df.columns else pd.Series([None]*len(df))

#     clean_df = pd.DataFrame(clean)
#     clean_df['BEGIN_DT'] = clean_df['BEGIN DATE'].apply(normalize_and_parse)
#     clean_df['END_DT'] = clean_df['END DATE'].apply(normalize_and_parse)

#     now_local = datetime.now(tz=SERVER_TZ)
#     now_utc = now_local.astimezone(zoneinfo.ZoneInfo('UTC'))

#     def is_active(row):
#         b, e = row['BEGIN_DT'], row['END_DT']
#         return bool(b and e and b <= now_utc <= e)

#     clean_df['active_now'] = clean_df.apply(is_active, axis=1)

#     # âœ… Replace problematic NaN/inf values
#     clean_df = clean_df.replace([np.nan, np.inf, -np.inf, pd.NaT], None)

#     # ðŸ§¹ Smart cleanup: remove empty and footer/metadata rows
#     original_row_count = len(clean_df)

#     def is_footer_row(row):
#         combined = " ".join(str(v) for v in row.values if v is not None).lower()
#         footer_patterns = [
#             r"copyright", r"all rights reserved", r"gardaworld", r"utc",
#             r"\b\d{1,2}-[a-z]{3}-\d{4}\b"
#         ]
#         return any(re.search(p, combined) for p in footer_patterns)

#     clean_df = clean_df.dropna(how="all")
#     clean_df = clean_df[~clean_df.apply(is_footer_row, axis=1)]
#     clean_df = clean_df[
#         clean_df["FIRST NAME"].notna() |
#         clean_df["LAST NAME"].notna() |
#         clean_df["EMAIL"].notna()
#     ]

#     removed_rows = original_row_count - len(clean_df)

#     def row_to_obj(i, row):
#         return {
#             'index': int(i),
#             'agency_id': row.get('AGENCY ID'),
#             'agency_name': row.get('AGENCY NAME'),
#             'first_name': row.get('FIRST NAME'),
#             'last_name': row.get('LAST NAME'),
#             'emp_id': row.get('EMP ID'),
#             'email': row.get('EMAIL'),
#             'pnr': row.get('PNR'),
#             'leg_type': row.get('LEG TYPE'),
#             'begin_dt': row.get('BEGIN_DT').isoformat() if row.get('BEGIN_DT') else None,
#             'end_dt': row.get('END_DT').isoformat() if row.get('END_DT') else None,
#             'from_location': row.get('FROM LOCATION'),
#             'from_country': row.get('FROM COUNTRY'),
#             'to_location': row.get('TO LOCATION'),
#             'to_country': row.get('TO COUNTRY'),
#             'active_now': bool(row.get('active_now'))
#         }

#     items = [row_to_obj(i, r) for i, r in clean_df.iterrows()]

#     summary = {
#         'rows_received': len(clean_df),
#         'rows_removed_as_footer_or_empty': removed_rows,
#         'rows_with_parse_errors': int(clean_df['BEGIN_DT'].isna().sum() + clean_df['END_DT'].isna().sum()),
#         'active_now_count': int(clean_df['active_now'].sum())
#     }

#     # âœ… Save to memory for future reuse
#     previous_data["summary"] = summary
#     previous_data["items"] = items

#     return JSONResponse(content={'summary': summary, 'items': items, 'message': 'New file uploaded and processed'})


# //////////////////////////









@app.post("/upload")
async def upload_excel(file: UploadFile = File(None)):
    global previous_data

    if file is None:
        if previous_data["items"] is not None:
            return JSONResponse(content={
                "summary": previous_data["summary"],
                "items": previous_data["items"],
                "message": "Returned previously uploaded data (no new file uploaded)"
            })
        else:
            raise HTTPException(status_code=400, detail="No file uploaded and no previous data found.")

    if not file.filename.lower().endswith((".xlsx", ".xls", ".csv")):
        raise HTTPException(status_code=400, detail="Unsupported file type")

    content = await file.read()
    file_stream = BytesIO(content)

    # ðŸ” Expected canonical headers
    expected_headers = [
        "AGENCY ID","AGENCY NAME","LAST NAME","FIRST NAME","TRAVELER",
        "EMP ID","EMAIL","PNR","LEG TYPE","BEGIN DATE","FROM LOCATION",
        "FROM COUNTRY","END DATE","TO LOCATION","TO COUNTRY"
    ]

    # --- STEP 1: Find header row dynamically ---
    if file.filename.lower().endswith(".csv"):
        text = content.decode(errors="ignore").splitlines()
        header_row_idx = None
        for i, line in enumerate(text[:30]):  # check first 30 lines
            for h in expected_headers:
                if h.lower() in line.lower():
                    header_row_idx = i
                    break
            if header_row_idx is not None:
                break

        if header_row_idx is None:
            raise HTTPException(status_code=400, detail="No valid header row found in CSV.")

        df = pd.read_csv(io.StringIO("\n".join(text)), skiprows=header_row_idx)
    else:
        # Excel â€” read first 30 rows to find header
        preview = pd.read_excel(file_stream, header=None, nrows=30)
        header_row_idx = None
        for i in range(len(preview)):
            row_values = " ".join(str(v).strip().lower() for v in preview.iloc[i].values if pd.notna(v))
            if any(h.lower() in row_values for h in expected_headers):
                header_row_idx = i
                break

        if header_row_idx is None:
            raise HTTPException(status_code=400, detail="No valid header row found in Excel.")

        # re-read from detected header row
        file_stream.seek(0)
        df = pd.read_excel(file_stream, header=header_row_idx)

    # --- STEP 2: Normalize column names ---
    df.columns = [str(c).strip() for c in df.columns]




@app.get("/data")
async def get_previous_data():
    """Return previously uploaded data if available."""
    if previous_data["items"] is not None:
        return JSONResponse(content={
            "summary": previous_data["summary"],
            "items": previous_data["items"],
            "message": "Loaded saved data from memory"
        })
    else:
        raise HTTPException(status_code=404, detail="No previously uploaded data found.")
